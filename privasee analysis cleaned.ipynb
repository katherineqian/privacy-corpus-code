{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text\n",
    "reading in files from my own local dirs, all of these directories are available in the repo so please replace the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_lib = \"/Users/katherineqian/Desktop/rsch/Privacy Policy Corpus/consolidation/threshold-0.5-overlap-similarity/\"\n",
    "text_lib = \"/Users/katherineqian/Desktop/rsch/Privacy Policy Corpus/txt_policies/\"\n",
    "desktop = \"/Users/katherineqian/Desktop/\"\n",
    "allfiles = [f[:-4] for f in listdir(annotations_lib) if isfile(join(annotations_lib, f))]\n",
    "\n",
    "#learning_corpus = [\"20_theatlantic.com\", \"21_imdb.com\", \"26_nytimes.com\", \"32_voxmedia.com\", \"33_nbcuniversal.com\", \"58_esquire.com\", \"59_liquor.com\", \"70_meredith.com\", \"82_sheknows.com\", \"93_pbs.org\"]\n",
    "\n",
    "learning_corpus = allfiles[:85]\n",
    "#learning_corpus = [\"928_stlouisfed.org\"]\n",
    "testing_corpus = allfiles[85:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec model generation\n",
    "I did the TFIDF and doc2vec separately. Some of the text preprocessing could be consolidated, but I kept them separate here. You can run the doc2vec code once and export the models to \".model\" files, then use those model files for future calculations by importing them to get more consistent results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a dict of lists of the tokens. \n",
    "'policyname': ['split', 'tokens', 'apple', etc.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_split = {}\n",
    "\n",
    "for policy in allfiles:\n",
    "    with open(text_lib + policy + \".txt\", 'r', encoding=\"utf8\", errors='ignore') as infile:\n",
    "        clean_text = re.sub('[\\n]', ' ', infile.read())\n",
    "        #strip URLs\n",
    "        clean_text = re.sub('https?:\\/\\/','<URL>', clean_text)\n",
    "        # strip html tags\n",
    "        clean_text = re.sub('<br>|<ul>|<li>|</ul>|</li>|<strong>|</strong>', '', clean_text)\n",
    "        # strip random stuff i dont want lol\n",
    "        clean_text = re.sub('[\\ufeff]', '', clean_text)\n",
    "        clean_text = re.sub('[\"\"]', '', clean_text)\n",
    "\n",
    "        # split by ||| delimiter\n",
    "        partokens = clean_text.split(\"|||\")\n",
    "        policies_split[policy] = partokens\n",
    "\n",
    "#print(policies_split)\n",
    "#print(policies_split['21_imdb.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting the data.\n"
     ]
    }
   ],
   "source": [
    "# scrapped using panda because making dataframe for each csv is too slow to copy\n",
    "# https://stackoverflow.com/questions/31674557/how-to-append-rows-in-a-pandas-dataframe-in-a-for-loop\n",
    "\n",
    "columns = ['paragraph_num', 'rawtxt', 'label', 'policy']\n",
    "chunks_tagged = []\n",
    "# getting raw sentence from the txt files\n",
    "now = 0\n",
    "prev = None\n",
    "for policy in allfiles:\n",
    "    with open(annotations_lib + policy + \".csv\", mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for rows in reader:\n",
    "            paragraph_num = int(rows[4])\n",
    "            # OPTIONAL: ONLY TAKE FIRST ANNOTATION\n",
    "            now = paragraph_num\n",
    "            if now == prev:\n",
    "                continue\n",
    "                \n",
    "            temp = [paragraph_num, policies_split[policy][paragraph_num], rows[5], policy]\n",
    "            chunks_tagged.append(temp)\n",
    "            prev = now\n",
    "            \n",
    "\n",
    "alldf = pd.DataFrame(chunks_tagged, columns=columns)\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# standard tokenizing function\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "tags_indexed = {'First Party Collection/Use': 1 , 'Third Party Sharing/Collection': 2, 'User Choice/Control': 3, 'User Access, Edit and Deletion': 4, 'Data Retention': 5, 'Data Security': 6, 'Policy Change': 7, 'Do Not Track': 8, 'International and Specific Audiences': 9, 'Introductory/Generic': 10, 'Practice Not Covered': 11, 'Privacy Contact Information': 12, 'Other': 13}\n",
    "tags_backward = {1:'First Party Collection/Use' , 2: 'Third Party Sharing/Collection', 3: 'User Choice/Control', 4: 'User Access, Edit and Deletion', 5: 'Data Retention', 6: 'Data Security', 7: 'Policy Change', 8: 'Do Not Track', 9: 'International and Specific Audiences', 10: 'Introductory/Generic', 11: 'Practice Not Covered', 12: 'Privacy Contact Information', 13: 'Other'}\n",
    "\n",
    "def create_tagged_tokens(row, dest):\n",
    "    tokens = tokenize_text(row['rawtxt'])\n",
    "    label = row['label']\n",
    "    # change tagged document to tuple because this application has no classifier\n",
    "    policy = row['policy']\n",
    "    dest.append(TaggedDocument(words = tokens, tags = [tags_indexed.get(label, 13)]))\n",
    "    return row\n",
    "\n",
    "def tokens_with_policy(row, dest):\n",
    "    tokens = tokenize_text(row['rawtxt'])\n",
    "    label = row['label']\n",
    "    # change tagged document to tuple because this application has no classifier\n",
    "    policy = row['policy']\n",
    "    dest.append((tokens, tags_indexed.get(label, 13), policy))\n",
    "    return row\n",
    "\n",
    "all_docs_tagged = []\n",
    "all_docs = []\n",
    "\n",
    "alldf.apply(create_tagged_tokens, axis = 1, args=[all_docs_tagged])\n",
    "alldf.apply(tokens_with_policy, axis = 1, args=[all_docs])\n",
    "print(\"Finished converting the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['privacy', 'policy'], 13, '701_tangeroutlet.com')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_num</th>\n",
       "      <th>rawtxt</th>\n",
       "      <th>label</th>\n",
       "      <th>policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Effective Date: May 7, 2015   Kraft Site Priva...</td>\n",
       "      <td>Other</td>\n",
       "      <td>746_kraftrecipes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Information We Collect   Personally-Identifia...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>746_kraftrecipes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Registration   To take full advantage of our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>746_kraftrecipes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Contests, Sweepstakes and Games   We may prov...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>746_kraftrecipes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>E-mail   When you ask us to send you recipes,...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>746_kraftrecipes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3785</td>\n",
       "      <td>24</td>\n",
       "      <td>If you no longer wish to receive our announcem...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1545_taylorswift.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3786</td>\n",
       "      <td>25</td>\n",
       "      <td>What is our policy on tracking?   You may be ...</td>\n",
       "      <td>Do Not Track</td>\n",
       "      <td>1545_taylorswift.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3787</td>\n",
       "      <td>26</td>\n",
       "      <td>We do not participate in tracking networks and...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1545_taylorswift.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3788</td>\n",
       "      <td>27</td>\n",
       "      <td>Does this Policy apply in other countries?   ...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1545_taylorswift.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3789</td>\n",
       "      <td>28</td>\n",
       "      <td>How can you contact us with questions about t...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1545_taylorswift.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3790 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paragraph_num                                             rawtxt  \\\n",
       "0                 0  Effective Date: May 7, 2015   Kraft Site Priva...   \n",
       "1                 1   Information We Collect   Personally-Identifia...   \n",
       "2                 2   Registration   To take full advantage of our ...   \n",
       "3                 3   Contests, Sweepstakes and Games   We may prov...   \n",
       "4                 4   E-mail   When you ask us to send you recipes,...   \n",
       "...             ...                                                ...   \n",
       "3785             24  If you no longer wish to receive our announcem...   \n",
       "3786             25   What is our policy on tracking?   You may be ...   \n",
       "3787             26  We do not participate in tracking networks and...   \n",
       "3788             27   Does this Policy apply in other countries?   ...   \n",
       "3789             28   How can you contact us with questions about t...   \n",
       "\n",
       "                           label                policy  \n",
       "0                          Other  746_kraftrecipes.com  \n",
       "1     First Party Collection/Use  746_kraftrecipes.com  \n",
       "2     First Party Collection/Use  746_kraftrecipes.com  \n",
       "3     First Party Collection/Use  746_kraftrecipes.com  \n",
       "4     First Party Collection/Use  746_kraftrecipes.com  \n",
       "...                          ...                   ...  \n",
       "3785                       Other  1545_taylorswift.com  \n",
       "3786                Do Not Track  1545_taylorswift.com  \n",
       "3787                       Other  1545_taylorswift.com  \n",
       "3788                       Other  1545_taylorswift.com  \n",
       "3789                       Other  1545_taylorswift.com  \n",
       "\n",
       "[3790 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_docs[85])\n",
    "alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "alldf.groupby('label').rawtxt.count().plot.bar(ylim=0)\n",
    "#print(testdf.groupby('label').rawtxt.count())\n",
    "#plt.savefig('traindistribution.png', dpi=100)\n",
    "plt.show()\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models (do not need to touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3791/3791 [00:00<00:00, 2682741.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#model_dbow = Doc2Vec(dm=1, alpha=0.025, min_alpha=0.025)\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_docs_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on epoch 0\n",
      "working on epoch 2\n",
      "working on epoch 4\n",
      "working on epoch 6\n",
      "working on epoch 8\n",
      "working on epoch 10\n",
      "working on epoch 12\n",
      "working on epoch 14\n",
      "working on epoch 16\n",
      "working on epoch 18\n",
      "working on epoch 20\n",
      "working on epoch 22\n",
      "working on epoch 24\n",
      "working on epoch 26\n",
      "working on epoch 28\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    if epoch % 2 == 0:\n",
    "        print('working on epoch', epoch)\n",
    "    model_dbow.train(utils.shuffle([x for x in all_docs_tagged]), total_examples=len(all_docs_tagged), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3791/3791 [00:00<00:00, 1678164.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on epoch 0\n",
      "working on epoch 2\n",
      "working on epoch 4\n",
      "working on epoch 6\n",
      "working on epoch 8\n",
      "working on epoch 10\n",
      "working on epoch 12\n",
      "working on epoch 14\n",
      "working on epoch 16\n",
      "working on epoch 18\n",
      "working on epoch 20\n",
      "working on epoch 22\n",
      "working on epoch 24\n",
      "working on epoch 26\n",
      "working on epoch 28\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=0, dm_mean=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "\n",
    "model_dmm.build_vocab([x for x in tqdm(all_docs_tagged)])\n",
    "for epoch in range(30):\n",
    "    if epoch % 2 == 0:\n",
    "        print('working on epoch', epoch)\n",
    "    model_dmm.train(utils.shuffle([x for x in all_docs_tagged]), total_examples=len(all_docs), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete temporary training data to free up RAM\n",
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: in my thesis the concatenated model performed best compared to both the dbow and dmm models when using logistic regression to classify text into categories. Here is a copy of the results (PV = paragraph vector)\n",
    "\n",
    "| Model | Accuracy |\n",
    "| :--- | --- |\n",
    "| PV-DM | 59.53% |\n",
    "| PV-DBOW | 59.07% |\n",
    "| Concatenated Model | 61.36% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate 2 models\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "concat = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fun testing part, you can type below\n",
    "# this currently doesn't work because we didn't use a classifier for this but useful to double check vector shape\n",
    "rawtest = \"security rocks our socks and we secure data seriously\"\n",
    "\n",
    "blah = tokenize_text(rawtest)\n",
    "vector = concat.infer_vector(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models you can import later\n",
    "model_dbow.save(\"100420_dbow.model\")\n",
    "model_dmm.save(\"100420_dmm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vectors\n",
    "new method 09/22 - bag of words + vectors:\n",
    "* corpus \n",
    "* policy\n",
    "* categories\n",
    "* categories within policies\n",
    "* every text segment in policies\n",
    "\n",
    "compare policy to corpus, categories in policies to categories, text segments to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bag = []\n",
    "category_bags = {category: [] for category in range(1,14)} \n",
    "category_vectors = {category: [] for category in range(1,14)} \n",
    "\n",
    "for doc in all_docs[1:]:\n",
    "    corpus_bag.extend(doc[0])\n",
    "    category_bags[doc[1]].extend(doc[0])\n",
    "\n",
    "# find overall corpus vector\n",
    "corpus_vector = concat.infer_vector(corpus_bag)\n",
    "\n",
    "# find vector for each category\n",
    "for idx in category_bags:\n",
    "    category_vectors[idx] = concat.infer_vector(category_bags[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = [\"531_archives.gov\", \"1306_chasepaymentech.com\", \"517_kaleidahealth.org\", \"701_tangeroutlet.com\", \"1618_sltrib.com\", \"1683_dailynews.com\"]\n",
    "test_list = [\"531_archives.gov\",\"1306_chasepaymentech.com\", \"517_kaleidahealth.org\", \"701_tangeroutlet.com\", \"1683_dailynews.com\", \"1618_sltrib.com\",\"591_google.com\",\"105_amazon.com\", \"303_reddit.com\", \"1300_bankofamerica.com\", \"1470_steampowered.com\", \"135_instagram.com\", \"26_nytimes.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701_tangeroutlet.com\n",
      "tokens: 45\n",
      "1618_sltrib.com\n",
      "tokens: 609\n",
      "1683_dailynews.com\n",
      "tokens: 629\n",
      "1300_bankofamerica.com\n",
      "tokens: 710\n",
      "26_nytimes.com\n",
      "tokens: 750\n",
      "303_reddit.com\n",
      "tokens: 742\n",
      "135_instagram.com\n",
      "tokens: 632\n",
      "1470_steampowered.com\n",
      "tokens: 422\n",
      "517_kaleidahealth.org\n",
      "tokens: 824\n",
      "591_google.com\n",
      "tokens: 604\n",
      "105_amazon.com\n",
      "tokens: 645\n",
      "1306_chasepaymentech.com\n",
      "tokens: 684\n",
      "531_archives.gov\n",
      "tokens: 861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sims = {}\n",
    "# each policy is a key, list is value with index corresponding to sentence no.\n",
    "sentence_sims = {\"746_kraftrecipes.com\":[]}\n",
    "\n",
    "# similarity of sentences in each category\n",
    "category_sims = {category: [] for category in range(1,14)} \n",
    "\n",
    "# bag of words for entire policy\n",
    "policy_bag = []\n",
    "\n",
    "# bag of words for every category within a policy\n",
    "intra_policy_bags = {\"746_kraftrecipes.com\": {i:[] for i in range(1,14)}}\n",
    "\n",
    "sentence_count = 0\n",
    "\n",
    "previous_policy = \"746_kraftrecipes.com\" # starting policy LOL\n",
    "# assume that all of the docs are in order... which they are but still... kinda jank\n",
    "\n",
    "for paragraph in all_docs[1:]:\n",
    "    current_policy = paragraph[2]\n",
    "        \n",
    "    if current_policy == previous_policy:\n",
    "        policy_bag.extend(paragraph[0])\n",
    "        intra_policy_bags[current_policy][paragraph[1]].extend(paragraph[0])\n",
    "        \n",
    "        # calculate similarity to category\n",
    "        sentence_vector = concat.infer_vector(paragraph[0])\n",
    "        cat_sim = cosine_similarity(sentence_vector.reshape(1, -1), category_vectors[paragraph[1]].reshape(1, -1))[0][0]\n",
    "        sentence_sims[current_policy].append((cat_sim, tags_backward[paragraph[1]], policies_split[current_policy][sentence_count]))\n",
    "        category_sims[paragraph[1]].append((cat_sim, current_policy, policies_split[current_policy][sentence_count]))\n",
    "        \n",
    "        sentence_count += 1\n",
    "        \n",
    "    else: # new policy, calculate old stuff then move on\n",
    "        policy_vector = concat.infer_vector(policy_bag)\n",
    "        cos_sims[previous_policy] = cosine_similarity(policy_vector.reshape(1, -1), corpus_vector.reshape(1, -1))[0][0]\n",
    "        if previous_policy in set(test_list):\n",
    "            print(previous_policy)\n",
    "            print(\"tokens:\", len(set(policy_bag)))\n",
    "        \n",
    "        sentence_count = 0\n",
    "        policy_bag = []\n",
    "        # move on\n",
    "        previous_policy = current_policy\n",
    "        policy_bag.extend(paragraph[0])\n",
    "        # calculate similarity to category\n",
    "        sentence_vector = concat.infer_vector(paragraph[0])\n",
    "        cat_sim = cosine_similarity(sentence_vector.reshape(1, -1), category_vectors[paragraph[1]].reshape(1, -1))[0][0]\n",
    "        sentence_sims[current_policy] = [(cat_sim, tags_backward[paragraph[1]], policies_split[current_policy][sentence_count])]\n",
    "        category_sims[paragraph[1]].append((cat_sim, current_policy, policies_split[current_policy][sentence_count]))\n",
    "        \n",
    "        # add new element to intra_policy_bag\n",
    "        intra_policy_bags[current_policy] = {i:[] for i in range(1,14)}\n",
    "        intra_policy_bags[current_policy][paragraph[1]].extend(paragraph[0])\n",
    "        \n",
    "        sentence_count += 1\n",
    "        \n",
    "# need to do the last one too lol\n",
    "policy_vector = concat.infer_vector(policy_bag)\n",
    "cos_sims[current_policy] = cosine_similarity(policy_vector.reshape(1, -1), corpus_vector.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks have been deactivated because they were not used in the demo version of PrivaSee, but their output was represented in the google sheet https://docs.google.com/spreadsheets/d/1K1uinuTpUnvUkCAQUy5s2FLF99PX53ipARHwxSFEAJU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find most and least similar text segments in each policy\n",
    "# bottom 20% of the policy not overall\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "for policy in test_small:\n",
    "    print(policy)\n",
    "    sim_list = sorted(sentence_sims[policy], reverse=True)\n",
    "    bottom_20_percent = ceil(0.2*len(sim_list))\n",
    "    print(bottom_20_percent)\n",
    "    \n",
    "    for i in range(bottom_20_percent):\n",
    "        print(sim_list[i][0])\n",
    "        \n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find most and least similar text segments for each category\n",
    "for idx in category_sims:\n",
    "    if len(category_sims[idx]) > 0:\n",
    "        #print(\"CATEGORY:\", tags_backward[idx])\n",
    "        sim_list = sorted(category_sims[idx])\n",
    "        #print(\"BOTTOM\")\n",
    "        #for i in range(5):\n",
    "            #print(sim_list[i][0])\n",
    "\n",
    "        print()\n",
    "        for i in sim_list[-5:]:\n",
    "            print(i[2])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find average similarity per category for list of policies\n",
    "\n",
    "for name in test_list:\n",
    "    print(name)\n",
    "    intra_category = {tags_backward[i]:[] for i in range(1,14)}\n",
    "    for segment in sentence_sims[name]:\n",
    "        intra_category[segment[1]].append(segment[0])\n",
    "\n",
    "    for idx in intra_category:\n",
    "        if idx == 'Introductory/Generic' or idx == 'Practice Not Covered' or idx == 'Privacy Contact Information':\n",
    "            continue\n",
    "        total = sum(intra_category[idx])\n",
    "        if total > 0:\n",
    "            print(total/len(intra_category[idx]))\n",
    "            print()\n",
    "        else:\n",
    "            print(0)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531_archives.gov\n",
      "0.6136999\n",
      "0.6634872\n",
      "0\n",
      "0\n",
      "0.74270564\n",
      "0.8206094\n",
      "0\n",
      "0\n",
      "0\n",
      "0.75197756\n",
      "\n",
      "1306_chasepaymentech.com\n",
      "0.5597743\n",
      "0.62833047\n",
      "0.70591056\n",
      "0\n",
      "0\n",
      "0.81791174\n",
      "0.71325094\n",
      "0\n",
      "0.7361982\n",
      "0.59707355\n",
      "\n",
      "517_kaleidahealth.org\n",
      "0.50714105\n",
      "0.6410389\n",
      "0.58669525\n",
      "0.70847034\n",
      "0\n",
      "0.6269293\n",
      "0.7572168\n",
      "0\n",
      "0\n",
      "0.4751174\n",
      "\n",
      "701_tangeroutlet.com\n",
      "0.55557406\n",
      "0\n",
      "0.63928235\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.5990204\n",
      "\n",
      "1683_dailynews.com\n",
      "0.7208432\n",
      "0.77096224\n",
      "0.7060659\n",
      "0.64898163\n",
      "0\n",
      "0.6583483\n",
      "0.75299823\n",
      "0.7854383\n",
      "0.8737706\n",
      "0.71200615\n",
      "\n",
      "1618_sltrib.com\n",
      "0.75591624\n",
      "0.78170407\n",
      "0.7036631\n",
      "0.66389346\n",
      "0\n",
      "0.6914846\n",
      "0.74871695\n",
      "0\n",
      "0.888137\n",
      "0.6786584\n",
      "\n",
      "591_google.com\n",
      "0.6554104\n",
      "0.78421724\n",
      "0.6218839\n",
      "0.6291423\n",
      "0.66511226\n",
      "0.731598\n",
      "0.70377076\n",
      "0\n",
      "0\n",
      "0.7499566\n",
      "\n",
      "105_amazon.com\n",
      "0.68426275\n",
      "0.79709554\n",
      "0.7424993\n",
      "0.6764031\n",
      "0.61662054\n",
      "0.78161275\n",
      "0.78139186\n",
      "0\n",
      "0.6609057\n",
      "0.7185467\n",
      "\n",
      "303_reddit.com\n",
      "0.6409879\n",
      "0.7552426\n",
      "0.65746975\n",
      "0.61908066\n",
      "0.83510196\n",
      "0.74038315\n",
      "0.7443948\n",
      "0.78059757\n",
      "0.814675\n",
      "0.7313578\n",
      "\n",
      "1300_bankofamerica.com\n",
      "0.6468332\n",
      "0.67336357\n",
      "0.69497126\n",
      "0\n",
      "0\n",
      "0.68758386\n",
      "0.74869156\n",
      "0\n",
      "0.7325946\n",
      "0.7957365\n",
      "\n",
      "1470_steampowered.com\n",
      "0.6448115\n",
      "0.7318653\n",
      "0.63200176\n",
      "0.65278137\n",
      "0\n",
      "0\n",
      "0.731328\n",
      "0\n",
      "0.74046636\n",
      "0.6953958\n",
      "\n",
      "135_instagram.com\n",
      "0.6439483\n",
      "0.864836\n",
      "0.63481057\n",
      "0\n",
      "0.6686776\n",
      "0.69024646\n",
      "0.72595483\n",
      "0\n",
      "0.75072765\n",
      "0.83117056\n",
      "\n",
      "26_nytimes.com\n",
      "0.7082816\n",
      "0.8586831\n",
      "0.68964773\n",
      "0.65783274\n",
      "0.59610844\n",
      "0.72372484\n",
      "0.7247925\n",
      "0\n",
      "0.77665603\n",
      "0.7790655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# infer and compare categories in a policy to the overall category vector.\n",
    "# this produces numbers in \"global corpus view\" in current demo\n",
    "\n",
    "for name in test_list:\n",
    "    current_pol = intra_policy_bags[name]\n",
    "    print(name)\n",
    "    for idx in current_pol:\n",
    "        # skip the blank categories\n",
    "        if idx == 10 or idx == 11 or idx == 12:\n",
    "            continue\n",
    "\n",
    "        #print(tags_backward[idx])\n",
    "        if current_pol[idx] == []:\n",
    "            print(0)\n",
    "            continue\n",
    "\n",
    "        vector = concat.infer_vector(current_pol[idx])\n",
    "        sim = cosine_similarity(vector.reshape(1, -1), category_vectors[idx].reshape(1, -1))[0][0]\n",
    "        print(sim)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print out a privacy policy with its sims\n",
    "\n",
    "for name in test_list:\n",
    "    print(name)\n",
    "    for sim in sentence_sims[name]:\n",
    "        print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optional (RawNBConvert, switch to code to use): can use to get a list of policies and their similarities to corpus in csv > create graph\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('cos_sims.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "    w = csv.DictWriter(f, cos_sims.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(cos_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit for method for TF-IDF analysis below goes to Kavita Ganesan's tutorial on extracting keywords with TF-IDF, at https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XoRjd257lTY\n",
    "\n",
    "(sort_coo is unchanged from the blog and find_tfidf is heavily based off her code. extract_topn_from vector is pretty much the same but I made some changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "        \n",
    "    #option: discard scores and just give keywords\n",
    "    #results=[]\n",
    "    #scores = []\n",
    "    #for idx in range(len(feature_vals)):\n",
    "    #    results.append(feature_vals[idx])\n",
    "    #    scores.append(score_vals[idx])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_strings = {\n",
    "    \"First Party Collection/Use\": \"\",\n",
    "    \"Third Party Sharing/Collection\": \"\", \n",
    "    \"User Choice/Control\": \"\", \n",
    "    \"User Access, Edit and Deletion\": \"\",\n",
    "    \"Data Retention\": \"\", \n",
    "    \"Data Security\": \"\",\n",
    "    \"Policy Change\": \"\", \n",
    "    \"Do Not Track\": \"\", \n",
    "    \"International and Specific Audiences\": \"\",\n",
    "    \"Other\": \"\"\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"First Party Collection/Use\": [],\n",
    "    \"Third Party Sharing/Collection\": [], \n",
    "    \"User Choice/Control\": [], \n",
    "    \"User Access, Edit and Deletion\": [],\n",
    "    \"Data Retention\": [], \n",
    "    \"Data Security\": [],\n",
    "    \"Policy Change\": [], \n",
    "    \"Do Not Track\": [], \n",
    "    \"International and Specific Audiences\": [],\n",
    "    \"Other\": []\n",
    "}\n",
    "\n",
    "def tuplify(n):\n",
    "    return (n, n)\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "    #categories[row['category']] += row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', max_features=10000)\n",
    "\n",
    "\n",
    "tfidf_vector = tfidf.fit_transform(df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "#policy = \"531_archives.gov\"\n",
    "\n",
    "def find_tfidf(policy):\n",
    "    tfidf_keywords = []\n",
    "    #unsimilar = {idx:[] for idx in categories}\n",
    "    \n",
    "    for sentence in policy_original[policy]:\n",
    "        doc = sentence[1]\n",
    "        ngram_num = tuplify(1)\n",
    "\n",
    "\n",
    "        # get the document that we want to extract keywords from\n",
    "\n",
    "        ## HER THING (kavita ganesan) p[##\n",
    "\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "        cv=CountVectorizer(max_df=0.85, ngram_range=ngram_num, stop_words='english',max_features=10000)\n",
    "        word_count_vector=cv.fit_transform(df.text)\n",
    "\n",
    "        feature_names=cv.get_feature_names()\n",
    "\n",
    "        tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "        #generate tf-idf for the given document\n",
    "        tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "        #sort the tf-idf vectors by descending order of scores\n",
    "        sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "        bottom_20_percent = ceil(0.2*len(sorted_items))\n",
    "\n",
    "        #extract only the top n; n here is 10\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,bottom_20_percent)\n",
    "        #unsimilar[category] = keywords\n",
    "\n",
    "        # now print the results\n",
    "        #print(category)\n",
    "        #print(\"Total Keywords:\", len(sorted_items))\n",
    "        #print(\"Bottom 20% Length:\", bottom_20_percent)\n",
    "\n",
    "        #print(\"\\n=====Doc=====\")\n",
    "        #print(doc)\n",
    "        #print(\"\\n===Keywords===\")\n",
    "        #print(keywords)\n",
    "\n",
    "        temp = set()\n",
    "        for k in keywords:\n",
    "            temp.add(k)\n",
    "            #print(k, keywords[k])\n",
    "        tfidf_keywords.append(temp)\n",
    "        #time.sleep(0.5)\n",
    "    return tfidf_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in TweetTokenizer().tokenize(sent):\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stem_tokenize_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tknzr = TweetTokenizer(preserve_case=False)\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in tknzr.tokenize(sent):\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(stemmer.stem(word))\n",
    "    return tokens\n",
    "\n",
    "def lemma_tokenize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tknzr = TweetTokenizer(preserve_case=False)\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in tknzr.tokenize(sent):\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(lemmatizer.lemmatize(word))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done :)\n"
     ]
    }
   ],
   "source": [
    "policy_bags = {}\n",
    "policy_original = {}\n",
    "\n",
    "# fill policy bags with tokenized sentences and sim scores from before\n",
    "for policy_name in sentence_sims:\n",
    "    for segment_no, segment in enumerate(sentence_sims[policy_name]):\n",
    "        sim_score = segment[0]\n",
    "        label = segment[1]\n",
    "        original_text = segment[2]\n",
    "        tokenized = lemma_tokenize_text(original_text)\n",
    "\n",
    "        tokenized_zip = (segment_no, tokenized, label)\n",
    "        original_zip = (segment_no, original_text, label, sim_score)\n",
    "        try:\n",
    "            policy_bags[policy_name].append(tokenized_zip)\n",
    "            policy_original[policy_name].append(original_zip)\n",
    "        except KeyError:\n",
    "            policy_bags[policy_name] = [tokenized_zip]\n",
    "            policy_original[policy_name] = [original_zip]\n",
    "\n",
    "        # fill category bags\n",
    "        categories[label].extend(tokenized)\n",
    "        \n",
    "print(\"done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own keyword counts + Geeks2Geeks bow\n",
    "\n",
    "keyword_counts = {}\n",
    "for category_name in categories:\n",
    "    word2count = {} \n",
    "    words = categories[category_name]\n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1\n",
    "    keyword_counts[category_name] = word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track',\n",
       " 'signal',\n",
       " 'browser',\n",
       " 'information',\n",
       " 'currently',\n",
       " 'dnt',\n",
       " 'respond',\n",
       " 'standard',\n",
       " 'website',\n",
       " 'user']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq \n",
    "# top ten are pretty similar across most categories, have \"information\" as top\n",
    "\n",
    "test_category = \"Do Not Track\"\n",
    "freq_words = heapq.nlargest(10, keyword_counts[test_category], key=keyword_counts[test_category].get)\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.58593079782671 139.35423119491043 -90.18236959925702\n"
     ]
    }
   ],
   "source": [
    "vals = list(word2count.values())\n",
    "std = np.std(vals)\n",
    "mean = np.mean(vals)\n",
    "print(mean, mean + std, mean - std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through words and count how many times each appears in sentences in that category.\n",
    "\n",
    "sentence_counts = {}\n",
    "\n",
    "for category_name in keyword_counts:\n",
    "    sent2count = {} \n",
    "    words = set(keyword_counts[category_name])\n",
    "    \n",
    "    # for each word in the set of keywords\n",
    "    for word in words: \n",
    "        \n",
    "        # for each policy\n",
    "        for policy_name in policy_bags:\n",
    "            \n",
    "            # for each sentence in policy\n",
    "            sentences = policy_bags[policy_name]\n",
    "            for sentence in sentences: \n",
    "                category = sentence[2]\n",
    "                # skip irrelevant cateogries\n",
    "                if category != category_name:\n",
    "                    continue\n",
    "                else: \n",
    "                    sentence_word_set = set(sentence[1])\n",
    "                    if word in sentence_word_set:\n",
    "                        if word not in sent2count.keys(): \n",
    "                            sent2count[word] = 1\n",
    "                        else: \n",
    "                            sent2count[word] += 1\n",
    "    sentence_counts[category_name] = sent2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "def print_sorted_dict(d):\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "#print(print_sorted_dict(sentence_counts[\"Do Not Track\"]))\n",
    "print(len(sentence_counts[\"Do Not Track\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_category = {\n",
    "    \"First Party Collection/Use\": 0,\n",
    "    \"Third Party Sharing/Collection\": 0, \n",
    "    \"User Choice/Control\": 0,\n",
    "    \"User Access, Edit and Deletion\": 0,\n",
    "    \"Data Retention\": 0,\n",
    "    \"Data Security\": 0,\n",
    "    \"Policy Change\": 0,\n",
    "    \"Do Not Track\": 0,\n",
    "    \"International and Specific Audiences\": 0,\n",
    "    \"Other\": 0\n",
    "}\n",
    "for policy_name in policy_bags:\n",
    "    # for each sentence in policy\n",
    "    sentences = policy_bags[policy_name]\n",
    "    for sentence in sentences: \n",
    "        category = sentence[2]\n",
    "        total_in_category[category] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "# note: I formatted this csv by hand later in excel (sort and formatting)\n",
    "with open('sentence_counts_new.csv', 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    for category in sentence_counts:\n",
    "        writer.writerow([category])\n",
    "        for key, value in sentence_counts[category].items():\n",
    "           writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of bottom 10% of appearance in sentences for example policies\n",
    "category_bottom_sets = {}\n",
    "for category in sentence_counts:\n",
    "    bottom = []\n",
    "    for key, value in sentence_counts[category].items():\n",
    "        if (value / total_in_category[category]) < 0.01:\n",
    "            bottom.append(key)\n",
    "    category_bottom_sets[category] = set(bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate HTML\n",
    "Yucky hard-coded ish code for generating the table HTML for each privacy policy. You can see that it looks like there are two copies of the same thing because one is the sorted version, and the switch just toggles between the two tables (original and sorted by ascending sim). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531_archives.gov\n",
      "Done\n",
      "517_kaleidahealth.org\n",
      "Done\n",
      "701_tangeroutlet.com\n",
      "Done\n",
      "1683_dailynews.com\n",
      "Done\n",
      "1618_sltrib.com\n",
      "Done\n",
      "591_google.com\n",
      "Done\n",
      "105_amazon.com\n",
      "Done\n",
      "303_reddit.com\n",
      "Done\n",
      "1300_bankofamerica.com\n",
      "Done\n",
      "1470_steampowered.com\n",
      "Done\n",
      "135_instagram.com\n",
      "Done\n",
      "26_nytimes.com\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#policy = \"531_archives.gov\"\n",
    "test_list = [\"531_archives.gov\", \"517_kaleidahealth.org\", \"701_tangeroutlet.com\", \"1683_dailynews.com\", \"1618_sltrib.com\",\"591_google.com\",\"105_amazon.com\", \"303_reddit.com\", \"1300_bankofamerica.com\", \"1470_steampowered.com\", \"135_instagram.com\", \"26_nytimes.com\"]\n",
    "\n",
    "for policy in test_list:\n",
    "    print(policy)\n",
    "    alltext=\"\"\n",
    "    temp=\"\"\n",
    "    tfidf_keywords = find_tfidf(policy)\n",
    "    \n",
    "    all_tokens_count = 0\n",
    "    bottom_tokens_count = {\n",
    "        \"First Party Collection/Use\": 0,\n",
    "        \"Third Party Sharing/Collection\": 0, \n",
    "        \"User Choice/Control\": 0,\n",
    "        \"User Access, Edit and Deletion\": 0,\n",
    "        \"Data Retention\": 0,\n",
    "        \"Data Security\": 0,\n",
    "        \"Policy Change\": 0,\n",
    "        \"Do Not Track\": 0,\n",
    "        \"International and Specific Audiences\": 0,\n",
    "        \"Other\": 0\n",
    "    }\n",
    "    \n",
    "    #print('<div class=\"contentA\">')\n",
    "    alltext += '<div class=\"contentA\">'\n",
    "    alltext += \"<table><tr><td>text</td><td>category</td><td>category-similarity</td></tr>\"\n",
    "    #print(\"<table><tr><td>text</td><td>category</td><td>category-similarity</td></tr>\")\n",
    "    #for sentence in sorted(policy_original[policy], key=lambda x: x[3]):\n",
    "    for sentence in policy_original[policy]:\n",
    "        \n",
    "        words = sentence[1].split()\n",
    "        category = sentence[2]\n",
    "        sim = round(float(sentence[3]), 3)\n",
    "        \n",
    "        if sim < 0.5999: \n",
    "            temp = \"<tr style='background-color:#FFF4CF'><td>\"\n",
    "        elif sim > 0.7:\n",
    "            temp = \"<tr style='background-color:#DCEDFD'><td>\"\n",
    "        else:\n",
    "            temp = \"<tr><td>\"\n",
    "            \n",
    "        for word in words:\n",
    "            #all_tokens_count += 1\n",
    "            if lemma_tokenize_text(word) == []:\n",
    "                temp += word + \" \"\n",
    "            else:\n",
    "                keyword = tokenize_text(word)[0] in tfidf_keywords[int(sentence[0])]\n",
    "                bottom_word = lemma_tokenize_text(word)[0] in category_bottom_sets[category]\n",
    "                \n",
    "                if keyword and bottom_word:\n",
    "                    temp += \"<b class='overlap'>\" + word + \"</b> \"\n",
    "                elif keyword:\n",
    "                    temp += \"<b class='keyword'>\" + word + \"</b> \"    \n",
    "                elif bottom_word:\n",
    "                    temp += \"<b class='bottomword'>\" + word + \"</b> \"\n",
    "                    bottom_tokens_count[category] += 1\n",
    "                else:\n",
    "                    temp += word + \" \"\n",
    "                \n",
    "        temp += \"</td><td>\" + category + \"</td><td>\" + str(sim) + \"</td></tr>\"\n",
    "        #print(temp)\n",
    "        alltext+=temp\n",
    "        #time.sleep(0.1)\n",
    "    #print(\"</table>\")\n",
    "    #print(\"</div>\")\n",
    "    #print('<div class=\"contentB\">')\n",
    "    #print(\"<table><tr><td>text</td><td>category</td><td>category-similarity</td></tr>\")\n",
    "    alltext += \"</table></div><div class='contentB'><table><tr><td>text</td><td>category</td><td>category-similarity</td></tr>\"\n",
    "    \n",
    "    for sentence in sorted(policy_original[policy], key=lambda x: x[3]):\n",
    "    #for sentence in policy_original[policy]:\n",
    "        words = sentence[1].split()\n",
    "        category = sentence[2]\n",
    "        sim = round(float(sentence[3]), 3)\n",
    "        \n",
    "        if float(sim) < 0.5999: \n",
    "            temp = \"<tr style='background-color:#FFF4CF'><td>\"\n",
    "        elif float(sim) > 0.7:\n",
    "            temp = \"<tr style='background-color:#DCEDFD'><td>\"\n",
    "        else:\n",
    "            temp = \"<tr><td>\"\n",
    "            \n",
    "        for word in words:\n",
    "            #all_tokens_count += 1\n",
    "            if lemma_tokenize_text(word) == []:\n",
    "                temp += word + \" \"\n",
    "            else:\n",
    "                keyword = tokenize_text(word)[0] in tfidf_keywords[int(sentence[0])]\n",
    "                bottom_word = lemma_tokenize_text(word)[0] in category_bottom_sets[category]\n",
    "                \n",
    "                if keyword and bottom_word:\n",
    "                    temp += \"<b class='overlap'>\" + word + \"</b> \"\n",
    "                elif keyword:\n",
    "                    temp += \"<b class='keyword'>\" + word + \"</b> \"    \n",
    "                elif bottom_word:\n",
    "                    temp += \"<b class='bottomword'>\" + word + \"</b> \"\n",
    "                    bottom_tokens_count[category] += 1\n",
    "                else:\n",
    "                    temp += word + \" \"\n",
    "                \n",
    "        temp += \"</td><td>\" + category + \"</td><td>\" + str(sim) + \"</td></tr>\"\n",
    "        alltext += temp\n",
    "        #print(temp)\n",
    "        #time.sleep(0.1)\n",
    "    #print(\"</table>\")\n",
    "    #print(\"</div>\")\n",
    "    alltext += \"</table></div>\"\n",
    "    \n",
    "    text_file = open(policy+\"HTML.txt\", \"w\")\n",
    "    text_file.write(alltext)\n",
    "    text_file.close()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #print(\"ALL:\", all_tokens_count)\n",
    "    #print(\"BOTTOM:\", bottom_tokens_count)\n",
    "    #print(sum(bottom_tokens_count.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
